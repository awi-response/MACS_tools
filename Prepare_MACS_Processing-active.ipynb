{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to move raw MACS files based on extracted footprints files \n",
    "**author:** Ingmar Nitze, Tabea Rettelbach, Simon SchÃ¤ffler\n",
    "\n",
    "**contact:** ingmar.nitze@awi.de\n",
    "\n",
    "**version date:** 2022-03-30\n",
    "\n",
    "**repository and other tools** https://github.com/awi-response/MACS_tools\n",
    "\n",
    "**CONDA Environment** please use *MACS37* conda environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. Setup folder structure\n",
    "2. convert MACS files to TIFF with *mipps* including devignetting\n",
    "3. Rescale image values **Optional**\n",
    "4. Crop corners of images **Optional**\n",
    "5. Prepare nav files for Pix4d\n",
    "6. PostProcessing to CIR mosiac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_vector(raster_file, remove_raster_mask=False):\n",
    "    maskfile = TMP_MASK_VECTORIZE_DIR / (raster_file.stem + '_mask.tif')\n",
    "    mask_vector = TMP_MASK_VECTORIZE_DIR / (raster_file.stem + '_mask.geojson')\n",
    "\n",
    "    s_extract_mask = f'gdal_translate -ot Byte -b mask {raster_file} {maskfile}'\n",
    "    os.system(s_extract_mask)\n",
    "\n",
    "    s_polygonize_mask = f'{GDAL_POLYGONIZE} -f GeoJSON {maskfile} {mask_vector}'\n",
    "    os.system(s_polygonize_mask)\n",
    "    \n",
    "    # needs to be fixed - is not deleting at the moment\n",
    "    if remove_raster_mask:\n",
    "        os.remove(maskfile)\n",
    "    \n",
    "    return mask_vector\n",
    "\n",
    "def load_and_prepare_footprints(vector_file):\n",
    "    gdf = gpd.read_file(vector_file)\n",
    "    file_name_raster = Path(str(vector_file).replace('_mask.geojson', '.tif')).name\n",
    "    gdf['Orthomosaic'] = file_name_raster\n",
    "    gdf['DSM'] = file_name_raster.replace('_Ortho_', '_DSM_')\n",
    "    return gdf\n",
    "\n",
    "def merge_single_vector_files(gdf_list, outfile, site_name, date_local):\n",
    "    # Merge and save final footprints file\n",
    "    gdf_merged = gpd.GeoDataFrame(pd.concat(gdf_list))\n",
    "    gdf_merged = gdf_merged.set_crs(crs=gdf_list[0].crs).to_crs(epsg=4326)\n",
    "\n",
    "    gdf_merged = gdf_merged[gdf_merged['DN'] > 0]\n",
    "    gdf_merged = gdf_merged.drop(columns=['DN'])\n",
    "\n",
    "    gdf_merged['Site_name'] = site_name\n",
    "    gdf_merged['Date'] = date_local#f'{date_local[:4]}-{date_local[4:6]}-{date_local[6:]}'\n",
    "\n",
    "    gdf_merged.to_file(outfile)\n",
    "    \n",
    "    \n",
    "def parse_site_name(site_name):\n",
    "    region, site, site_number, date_tmp, resolution = site_name.split('_')\n",
    "    date = f'{date_tmp[:4]}-{date_tmp[4:6]}-{date_tmp[6:]}'\n",
    "    return region, site, site_number, date, resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* prefer full/absolute paths\n",
    "* Create processing template automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Set project directory here, where you want to process your data\n",
    "#PROJECT_DIR =  = Path(r'')\n",
    "PROJECT_DIR = Path(r'D:\\Pix4D_Processing\\tests\\Benchmark_Local_TUK')\n",
    "#PROJECT_DIR = r'D:\\pix4d_Processing\\ThawTrendAir_2019\\Image_Test_CODEscaleHi' # SET Project output\n",
    "\n",
    "#PIX4d_PROJECT_NAME = Path(PROJECT_DIR).name\n",
    "#PIX4d_PROJECT_NAME = 'Meade_merged'\n",
    "PIX4d_PROJECT_NAME = 'Benchmark_Local_TUK'\n",
    "\n",
    "# SET AOI for footprints selection\n",
    "#AOI = Path(r'')\n",
    "AOI = Path(r'S:\\p_macsprocessing\\test_sites\\site_definitions') / 'test_TUK.shp'\n",
    "#AOI = Path(r'D:\\Pix4D_Processing\\MACS_Projects\\Lakes_Baldwin\\02_studysites') / 'AOI.geojson'\n",
    "\n",
    "# determine which sensors to include in processing (possible options: 'left', 'right', 'nir')\n",
    "sensors = ['left', 'right', 'nir']\n",
    "\n",
    "# SET SCALING \n",
    "SCALING = 1\n",
    "SCALE_LOW = False # Set to True to use calculated lower boundary - skews NDVI\n",
    "SCALE_HIGH = True # Set to True to use calculated upper boundary\n",
    "SCALING_EMPIRICAL = False\n",
    "\n",
    "\n",
    "# Set CROP CORNER if \n",
    "CROP_CORNER = 0 # SET to 1 if you want to crop corners (set to NoData)\n",
    "DISK_SIZE = 5200 # Cropping diameter, the larger the fewer no data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import sys\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from joblib import delayed, Parallel, wrap_non_picklable_objects\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from processing_utils import *\n",
    "from utils_postprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = Path(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed Settings\n",
    "* These Settings can be kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2018 Canada\n",
    "PARENT_DIRS = [Path(r'N:\\response\\Restricted_Airborne\\MACs\\Canada\\1_MACS_original_images'),\n",
    "              Path(r'N:\\response\\Restricted_Airborne\\MACs\\Alaska\\ThawTrend-Air_2019\\raw_data'),\n",
    "              Path(r'N:\\response\\Restricted_Airborne\\MACs\\2021_Perma-X_Alaska\\01_raw_data')\n",
    "              ]\n",
    "              \n",
    "# imageing projects footprint file\n",
    "PROJECTS_FILES = [Path(r'N:\\response\\Restricted_Airborne\\MACs\\Canada\\0_MACS_important_files\\1_all_canada_flights_2018_footprints\\all_canada_flights_2018_footprints_datasets.shp'),\n",
    "                  Path(r'N:\\response\\Restricted_Airborne\\MACs\\Alaska\\ThawTrend-Air_2019\\footprints\\ThawTrendAir2019_MACS_footprints_datasets.shp'),\n",
    "                  Path(r'N:\\response\\Restricted_Airborne\\MACs\\2021_Perma-X_Alaska\\03_footprints\\Perma-X\\PermaX2021_MACS_footprints_datasets.shp')\n",
    "                 ]\n",
    "\n",
    "CODE_DIR = pwd\n",
    "MIPPS_DIR = r'C:\\Program Files\\DLR MACS-Box\\bin'\n",
    "MIPPS_BIN = r'..\\tools\\MACS\\mipps.exe'\n",
    "EXIF_PATH = Path(CODE_DIR / Path(r'exiftool\\exiftool.exe'))\n",
    "mipps_script_dir = Path('mipps_scripts')\n",
    "\n",
    "mipps_script_nir = '33552_all_taps_2018-09-26_12-58-15_modelbased.mipps'\n",
    "mipps_script_right = '33576_all_taps_2018-09-26_13-13-43_modelbased.mipps'\n",
    "mipps_script_left = '33577_all_taps_2018-09-26_13-21-24_modelbased.mipps'\n",
    "\n",
    "mipps_script_nir = pwd / mipps_script_dir / mipps_script_nir\n",
    "mipps_script_right = pwd / mipps_script_dir / mipps_script_right\n",
    "mipps_script_left = pwd / mipps_script_dir / mipps_script_left\n",
    "\n",
    "DATA_DIR = Path(PROJECT_DIR) / '01_rawdata' / 'tif'\n",
    "OUTDIR = {'right': DATA_DIR / Path('33576_Right'),\n",
    "          'left':DATA_DIR / Path('33577_Left'),\n",
    "          'nir':DATA_DIR / Path('33552_NIR')}\n",
    "tag = {'right':'MACS_RGB_Right_33576',\n",
    "       'left':'MACS_RGB_Left_33577',\n",
    "       'nir':'MACS_NIR_33552'}\n",
    "\n",
    "# Path of filtered footprints file (.shp file)\n",
    "path_footprints = Path(PROJECT_DIR) / '02_studysites' / 'footprints.shp'\n",
    "outdir = os.path.join(PROJECT_DIR, '01_rawdata','tif')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare processing dir \n",
    "* check if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zippath = os.path.join(CODE_DIR, 'processing_folder_structure_template.zip')\n",
    "nav_script_path = os.path.join(CODE_DIR, 'pix4dnav.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(zippath, 'r') as zip_ref:\n",
    "    zip_ref.extractall(PROJECT_DIR)\n",
    "shutil.copy(nav_script_path, outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger\n",
    "logging.basicConfig(filename=PROJECT_DIR / f'{PROJECT_DIR.name}.log', level=logging.INFO, format='%(asctime)s %(message)s', )\n",
    "logging.info('Creation of logfile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Selection of footprints\n",
    "provide more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Creating footprints selection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for projects_file, parent_dir in zip(PROJECTS_FILES, PARENT_DIRS):\n",
    "    dss = []\n",
    "    print(parent_dir.parent)\n",
    "    ds = get_overlapping_ds(AOI, projects_file, parent_dir)\n",
    "    #dss.append(ds)\n",
    "    if len(ds) > 0:\n",
    "        break\n",
    "stats = get_dataset_stats(ds, parent_dir, AOI)\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Dataset ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = input('Please select ID: ')\n",
    "footprints = retrieve_footprints(ds, dataset_id, parent_dir, AOI)\n",
    "print(\"Total number of images:\", len(footprints))\n",
    "print(\"NIR images:\", (footprints['Looking'] == 'center').sum())\n",
    "print(\"RGB right images:\", (footprints['Looking'] == 'right').sum())\n",
    "print(\"RGB left images:\", (footprints['Looking'] == 'left').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "footprints.to_file(path_footprints)\n",
    "logging.info(f'Footprints file save to {path_footprints}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load filtered footprints file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = get_dataset_name(ds, dataset_id)\n",
    "path_infiles = Path(parent_dir) / dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = prepare_df_for_mipps(path_footprints, path_infiles)\n",
    "df_final['full_path'] = df_final.apply(lambda x: f'\"{x.full_path}\"', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of images:\", len(df_final))\n",
    "print(\"NIR images:\", (df_final['Looking'] == 'center').sum())\n",
    "print(\"RGB right images:\", (df_final['Looking'] == 'right').sum())\n",
    "print(\"RGB left images:\", (df_final['Looking'] == 'left').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(MIPPS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_roll = 3 # Select maximum roll angle to avoid image issues - SET in main settings part?\n",
    "chunksize = 20 # this is a mipps-script thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export MACS to TIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f'Start exporting MACS files to TIFF using DLR mipps')\n",
    "logging.info(f\"Total number of images: {len(df_final)}\")\n",
    "logging.info(f\"NIR images: {(df_final['Looking'] == 'center').sum()}\")\n",
    "logging.info(f\"RGB right images: {(df_final['Looking'] == 'right').sum()}\")\n",
    "logging.info(f\"RGB left images:{(df_final['Looking'] == 'left').sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is relevant for NIR only\n",
    "if 'nir' in sensors:\n",
    "    logging.info(f'Start transforming NIR files')\n",
    "    logging.info(f'MIPPS Script: {mipps_script_nir.name}')\n",
    "    \n",
    "    looking = 'center'\n",
    "    q = (np.abs(df_final['Roll[deg]']) < max_roll) & (df_final['Looking'] == looking)\n",
    "    df_nir = df_final[q]\n",
    "    print(len(df_nir))\n",
    "    split = len(df_nir) // chunksize\n",
    "    if split == 0: split+=1\n",
    "    for df in tqdm.tqdm_notebook(np.array_split(df_nir, split)):\n",
    "        outlist = ' '.join(df['full_path'].values[:])\n",
    "        s = f'{MIPPS_BIN} -c={mipps_script_nir} -o={outdir} -j=4 {outlist}'\n",
    "        os.system(s)\n",
    "        #print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is RGB\n",
    "if 'right' in sensors:\n",
    "    logging.info(f'Start transforming RGB right files')\n",
    "    logging.info(f'MIPPS Script: {mipps_script_right.name}')\n",
    "    \n",
    "    looking = 'right'\n",
    "    q = (np.abs(df_final['Roll[deg]']) < max_roll) & (df_final['Looking'] == looking)\n",
    "    df_right = df_final[q]\n",
    "    split = len(df_right) // chunksize\n",
    "    if split == 0: split+=1\n",
    "    for df in tqdm.tqdm_notebook(np.array_split(df_right, split)):\n",
    "        outlist = ' '.join(df['full_path'].values[:])\n",
    "        s = f'{MIPPS_BIN} -c={mipps_script_right} -o={outdir} -j=4 {outlist}'\n",
    "        os.system(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'left' in sensors:\n",
    "    logging.info(f'Start transforming RGB left files')\n",
    "    logging.info(f'MIPPS Script: {mipps_script_left.name}')\n",
    "    \n",
    "    looking = 'left'\n",
    "    q = (np.abs(df_final['Roll[deg]']) < max_roll) & (df_final['Looking'] == looking)\n",
    "    df_left = df_final[q]\n",
    "    split = len(df_left) // chunksize\n",
    "    if split == 0: split+=1\n",
    "    for df in tqdm.tqdm_notebook(np.array_split(df_left, split)):\n",
    "        outlist = ' '.join(df['full_path'].values[:])\n",
    "        s = f'{MIPPS_BIN} -c={mipps_script_left} -o={outdir} -j=4 {outlist}'\n",
    "        os.system(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale image values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCALING:\n",
    "    logging.info(f'Start reading Image statistics')\n",
    "    %time df_stats = get_image_stats_multi(OUTDIR, sensors, nth_images=1, quiet=False, n_jobs=40)\n",
    "    #absolute\n",
    "    if SCALE_LOW:\n",
    "        scale_lower = int(df_stats['min'].mean().round())\n",
    "    else:\n",
    "        scale_lower = 1\n",
    "    if SCALE_HIGH:\n",
    "        scale_upper = int(df_stats['max'].mean().round())\n",
    "    else:\n",
    "        scale_upper = 2*16-1\n",
    "    print(f'Mean of minimums: {scale_lower}')\n",
    "    print(f'Mean of maximums: {scale_upper}')\n",
    "    logging.info(f'Mean of minimums: {scale_lower}')\n",
    "    logging.info(f'Mean of maximums: {scale_upper}')\n",
    "    logging.info(f'Finished reading Image statistics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCALING_EMPIRICAL:\n",
    "    # empirical\n",
    "    df_stats2 = df_stats.replace({'left':'RGB', 'right':'RGB'})\n",
    "    grouped = df_stats2.groupby('sensor').mean()\n",
    "    empirical_scale_factor = (grouped.loc['nir'] / grouped.loc['RGB'] / 0.8)['min']\n",
    "else: \n",
    "    empirical_scale_factor = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run scaling\n",
    "* minimum default to 1\n",
    "* consistency for final index calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCALING:\n",
    "    logging.info(f'Start Image Scaling')\n",
    "    n_jobs = 20\n",
    "    for sensor in sensors:\n",
    "        print(f'Processing {sensor}')\n",
    "        #shutter_factor\n",
    "        images = list(OUTDIR[sensor].glob('*.tif'))[:]\n",
    "        if sensor in ['right', 'left']:\n",
    "            shutter_factor = get_shutter_factor(OUTDIR, sensors) * empirical_scale_factor\n",
    "            print(f'RGB to NIR factor = {shutter_factor}')\n",
    "        else:\n",
    "            shutter_factor = 1\n",
    "        \n",
    "        %time _ = Parallel(n_jobs=n_jobs)(delayed(write_new_values)(image, scale_lower, scale_upper, shutter_factor=shutter_factor, tag=True) for image in tqdm.tqdm_notebook(images[:]))\n",
    "    logging.info(f'Finished Image Scaling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crop Corners of images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CROP_CORNER:\n",
    "    logging.info(f'Start Cropping corners')\n",
    "    logging.info(f'Disk Size: {DISK_SIZE}')\n",
    "    #mask = make_mask((3232, 4864), disksize=DISK_SIZE)\n",
    "    for sensor in sensors[:]:\n",
    "        mask = make_mask((3232, 4864), disksize=DISK_SIZE)\n",
    "        images = list(OUTDIR[sensor].glob('*'))\n",
    "        if sensor != 'nir':\n",
    "            mask = np.r_[[mask]*3]\n",
    "        %time _ = Parallel(n_jobs=4)(delayed(mask_and_tag)(image, mask, tag=None) for image in tqdm.tqdm_notebook(images))\n",
    "    logging.info(f'Finished Cropping corners')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write exif information into all images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f'Start writing EXIF Tags')\n",
    "for sensor in tqdm.tqdm_notebook(sensors):\n",
    "    print(sensor)\n",
    "    %time write_exif(OUTDIR[sensor], tag[sensor], EXIF_PATH)\n",
    "logging.info(f'Finished writing EXIF Tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f'Start preparing nav file')\n",
    "\n",
    "navfile = list(Path(path_infiles).glob('*nav.txt'))[0]\n",
    "shutil.copy(navfile, OUTDIR['nir'].parent / 'nav.txt')\n",
    "os.chdir(OUTDIR['nir'].parent)\n",
    "os.system('python pix4dnav.py')\n",
    "\n",
    "logging.info(f'Finished preparing nav file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Run Pix4d Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mosaic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiling and renaming\n",
    "* check pix4d tiling system\n",
    "  * using same system\n",
    "* create tiling check for pyramids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo:\n",
    "3. rename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PostProcess Optical\n",
    "* create 4 band mosaic + calculate pyramids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f'Start postprocessing Orthoimage tiles!')\n",
    "\n",
    "tiles_dir = Path(PROJECT_DIR) / '04_pix4d' / PIX4d_PROJECT_NAME / '3_dsm_ortho' / '2_mosaic' / 'tiles'\n",
    "flist = list(tiles_dir.glob('*.tif'))\n",
    "df = flist_to_df(flist)\n",
    "df['tile_id'] = df.apply(lambda x: x.row + '_' + x.col, axis=1)\n",
    "tiles = pd.unique(df['tile_id'])\n",
    "\n",
    "#### Run \n",
    "\n",
    "# Parallel version crashes\n",
    "%time _ = Parallel(n_jobs=40)(delayed(full_postprocessing_optical)(df, tile) for tile in tqdm.tqdm_notebook(tiles[:]))\n",
    "\n",
    "logging.info(f'Finished postprocessing Orthoimage tiles!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PostProcess DSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f'Start postprocessing DSM tiles!')\n",
    "\n",
    "tiles_dir_dsm = Path(PROJECT_DIR) / '04_pix4d' / PIX4d_PROJECT_NAME / '3_dsm_ortho' / '1_dsm' / 'tiles'\n",
    "flist_dsm = list(tiles_dir_dsm.glob('*.tif'))\n",
    "\n",
    "%time _ = Parallel(n_jobs=40)(delayed(calculate_pyramids)(filename) for filename in tqdm.tqdm_notebook(flist_dsm[:]))\n",
    "\n",
    "logging.info(f'Finished postprocessing DSM tiles!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename\n",
    "Move to settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WA_KobukDelta_02_20210710_20cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SITE_NAME = 'NWC_TukRoad_02_20180822_10cm'\n",
    "\n",
    "PRODUCT_DIR = Path(PROJECT_DIR) / '06_DataProducts'\n",
    "TARGET_DIR_ORTHO = tiles_dir = PRODUCT_DIR / 'Orthomosaic'\n",
    "TARGET_DIR_DSM = tiles_dir = PRODUCT_DIR / 'DSM'\n",
    "\n",
    "region, site, site_number, date, resolution = parse_site_name(SITE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_and_rename_processed_tiles(df, out_basename, target_dir, product_type, move=False):\n",
    "    for index, row in df.iloc[:].iterrows():\n",
    "        tile_id = row.tile_id\n",
    "        infile = row.filename\n",
    "        infile_ovr = Path(str(infile) + '.ovr')\n",
    "        outfile = target_dir / f'{out_basename}_{product_type}_{tile_id}.tif'\n",
    "        outfile_ovr = Path(str(outfile) + '.ovr')\n",
    "        #print(infile, outfile)\n",
    "        \n",
    "        if move:\n",
    "            shutil.move(infile, outfile)\n",
    "        else:\n",
    "            shutil.copy(infile, outfile)\n",
    "        try:\n",
    "            if move:\n",
    "                shutil.move(infile_ovr, outfile_ovr)\n",
    "            else:\n",
    "                shutil.copy(infile_ovr, outfile_ovr)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create output dirs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(TARGET_DIR_ORTHO, exist_ok=True)\n",
    "os.makedirs(TARGET_DIR_DSM, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move and rename to output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_dir = Path(PROJECT_DIR) / '04_pix4d' / PIX4d_PROJECT_NAME / '3_dsm_ortho' / '2_mosaic' / 'tiles'\n",
    "flist = list(tiles_dir.glob('mosaic*.tif'))\n",
    "df = flist_to_df(flist)\n",
    "df['tile_id'] = df.apply(lambda x: x.row + '_' + x.col, axis=1)\n",
    "tiles = pd.unique(df['tile_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_and_rename_processed_tiles(df, SITE_NAME, TARGET_DIR_ORTHO, 'Ortho', move=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DSM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_dir_dsm = Path(PROJECT_DIR) / '04_pix4d' / PIX4d_PROJECT_NAME / '3_dsm_ortho' / '1_dsm' / 'tiles'\n",
    "flist_dsm = list(tiles_dir_dsm.glob('*.tif'))\n",
    "df_dsm = flist_to_df(flist_dsm)\n",
    "df_dsm['tile_id'] = df_dsm.apply(lambda x: x.row + '_' + x.col, axis=1)\n",
    "tiles = pd.unique(df_dsm['tile_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_and_rename_processed_tiles(df_dsm, SITE_NAME, TARGET_DIR_DSM, 'DSM', move=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create footprints file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TMP_MASK_VECTORIZE_DIR = TARGET_DIR_ORTHO / 'TMP_MASK_VECTORIZE_DIR'.lower()\n",
    "TMP_MASK_VECTORIZE_DIR = PRODUCT_DIR / 'tmp_footprints'#Path(r'D:\\Pix4D_Processing\\test')\n",
    "os.makedirs(TMP_MASK_VECTORIZE_DIR, exist_ok=True)\n",
    "GDAL_POLYGONIZE = Path(os.environ['CONDA_PREFIX']) / 'Scripts' / 'gdal_polygonize.py'\n",
    "FOOTPRINTS_FILE = PRODUCT_DIR / f'{SITE_NAME}_tile_footprints.geojson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "401363bd5d1944f4b88c9a1a179fc5dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 203 ms\n",
      "Wall time: 5.91 s\n"
     ]
    }
   ],
   "source": [
    "# create vector mask of Data (DN=0 for noData, DN=255 for valid Data)\n",
    "flist_out = list(TARGET_DIR_ORTHO.glob('*.tif'))\n",
    "%time vector_list = Parallel(n_jobs=40)(delayed(create_mask_vector)(infile) for infile in tqdm.tqdm_notebook(flist_out[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae45ef4ff40b45f68d0b4ed8cf2c8e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 1.12 s\n"
     ]
    }
   ],
   "source": [
    "# Merge vectors and remove noData parts\n",
    "%time gdf_list = Parallel(n_jobs=40)(delayed(load_and_prepare_footprints)(vector_file) for vector_file in tqdm.tqdm_notebook(vector_list[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\initze\\.conda\\envs\\MACS37\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n"
     ]
    }
   ],
   "source": [
    "merge_single_vector_files(gdf_list, FOOTPRINTS_FILE, SITE_NAME, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup temporary dir\n",
    "shutil.rmtree(TMP_MASK_VECTORIZE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only full set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 4 band CIR mosaic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this part after Processing in Pix4D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_processed = Path(PROJECT_DIR) / '04_pix4d' / PIX4d_PROJECT_NAME / '3_dsm_ortho' / '2_mosaic'\n",
    "os.chdir(dir_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgbfile = list(Path('.').glob('*group1.tif'))[0]\n",
    "nirfile = list(Path('.').glob('*nir.tif'))[0]\n",
    "outmosaic = '_'.join(rgbfile.name.split('_')[:-3])+'_mosaic.tif'\n",
    "\n",
    "dsm_dir = dir_processed = Path(PROJECT_DIR) / '04_pix4d' / PIX4d_PROJECT_NAME / '3_dsm_ortho' / '1_dsm'\n",
    "dsm_file = list(dsm_dir.glob('*dsm.tif'))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export single bands and merge afterwards \n",
    "* make function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f'Start restructuring and mosaicking final Orthomosaics!')\n",
    "\n",
    "remove_files = False\n",
    "\n",
    "for band in [1,2,3]:\n",
    "    s = f'gdalbuildvrt -b {band} rgb_{band}.vrt {rgbfile}'\n",
    "    os.system(s)\n",
    "\n",
    "for band in [1]:\n",
    "    s = f'gdalbuildvrt -b {band} nir_{band}.vrt {nirfile}'\n",
    "    os.system(s)\n",
    "\n",
    "s = f'gdalbuildvrt -separate 4band.vrt rgb_3.vrt rgb_2.vrt rgb_1.vrt nir_1.vrt'\n",
    "os.system(s)\n",
    "\n",
    "s = f'gdal_translate -a_nodata 0 -co COMPRESS=DEFLATE -co BIGTIFF=YES 4band.vrt {outmosaic}'\n",
    "os.system(s)\n",
    "if remove_files:\n",
    "    for file in ['rgb_3.vrt', 'rgb_2.vrt', 'rgb_1.vrt', 'nir_1.vrt', '4band.vrt']:\n",
    "        os.remove(file)\n",
    "logging.info(f'Finished restructuring and mosaicking final Orthomosaics!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f'Start updating mask of Orthomosiacs!')\n",
    "with rasterio.open(outmosaic, 'r+') as src:\n",
    "    src.profile['nodata'] = 0\n",
    "    data = src.read()\n",
    "    newmask = ~(data == 0).any(axis=0)\n",
    "    newmask_write = np.r_[src.count * [newmask]]\n",
    "    data_masked = data * newmask_write\n",
    "    src.set_band_description(1, 'MACS Blue Band')\n",
    "    src.set_band_description(2, 'MACS Green Band')\n",
    "    src.set_band_description(3, 'MACS Red Band')\n",
    "    src.set_band_description(4, 'MACS NIR Band')\n",
    "    src.write(data_masked)\n",
    "logging.info(f'Finished updating mask of Orthomosiacs!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Pyramids "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ortho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f'Start calculating pyramid layers for Orthoimage!!')\n",
    "addo = f'gdaladdo -ro --config COMPRESS_OVERVIEW DEFLATE --config GDAL_NUM_THREADS ALL_CPUS {outmosaic}'\n",
    "os.system(addo)\n",
    "logging.info(f'Finished calculating pyramid layers for Orthoimage!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f'Start calculating pyramid layers for DSM!')\n",
    "addo = f'gdaladdo -ro --config COMPRESS_OVERVIEW DEFLATE --config GDAL_NUM_THREADS ALL_CPUS {dsm_file}'\n",
    "os.system(addo)\n",
    "logging.info(f'Finished calculating pyramid layers for DSM!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
